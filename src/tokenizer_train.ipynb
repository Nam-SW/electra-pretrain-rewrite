{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:31:14.620825Z",
     "start_time": "2022-03-30T01:31:14.317555Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:31:15.217775Z",
     "start_time": "2022-03-30T01:31:15.213682Z"
    },
    "id": "ahc_PV2F4WxX"
   },
   "outputs": [],
   "source": [
    "def dump_jsonl(data, output_path, append=False):\n",
    "    mode = \"a+\" if append else \"w\"\n",
    "    with open(output_path, mode, encoding=\"utf-8\") as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + \"\\n\")\n",
    "\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    data = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip(\"\\n|\\r\")))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:31:15.836942Z",
     "start_time": "2022-03-30T01:31:15.834513Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 32000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoCW3M3_HeeV"
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:30:18.874123Z",
     "start_time": "2022-02-28T01:30:18.868413Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_jsonl(\"../data/sample.json\")\n",
    "\n",
    "def processing(text):\n",
    "    text = re.sub(r\" +\", r\" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def gen():\n",
    "    for row in data:\n",
    "        for k, v in row.items():\n",
    "            if isinstance(v, str):\n",
    "                yield processing(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M78kc0QyHjW6"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:16.185253Z",
     "start_time": "2022-02-28T01:31:16.164451Z"
    }
   },
   "outputs": [],
   "source": [
    "user_defined_symbols = [\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\",\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "    \"[BOS]\",\n",
    "    \"[EOS]\",\n",
    "]\n",
    "unused_token_num = 100\n",
    "unused_list = [f\"[UNUSED{i}]\" for i in range(unused_token_num)]\n",
    "whole_user_defined_symbols = user_defined_symbols + unused_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:27.226189Z",
     "start_time": "2022-02-28T01:31:27.213352Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:38.583566Z",
     "start_time": "2022-02-28T01:31:38.578697Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:39.951116Z",
     "start_time": "2022-02-28T01:31:39.947288Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:41.273368Z",
     "start_time": "2022-02-28T01:31:41.268391Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(t, i) for i, t in enumerate(user_defined_symbols)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:44:00.048196Z",
     "start_time": "2022-02-28T01:31:42.250921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=whole_user_defined_symbols,\n",
    ")\n",
    "tokenizer.train_from_iterator(gen(), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:44:00.056369Z",
     "start_time": "2022-02-28T01:44:00.050181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 22692, 5213, 2158, 19113, 120, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'테스트를 목적으로 하는 문장 .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(\"테스트를 목적으로 하는 문장.\")\n",
    "print(output.ids)\n",
    "\n",
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:44:00.071199Z",
     "start_time": "2022-02-28T01:44:00.057864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'테스트를 목적으로 하는 문장.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders\n",
    "\n",
    "tokenizer.decoder = decoders.WordPiece()\n",
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert transformers tokenizer and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:30:09.301876Z",
     "start_time": "2022-03-30T01:30:08.171252Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizerFast\n",
    "\n",
    "\n",
    "hf_tokenizer = ElectraTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:46:03.504487Z",
     "start_time": "2022-02-28T01:46:03.496017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for token in user_defined_symbols:\n",
    "    setattr(hf_tokenizer, token[1:-1].lower(), token)\n",
    "\n",
    "special_tokens_dict = {\"additional_special_tokens\": user_defined_symbols}\n",
    "hf_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:46:06.966327Z",
     "start_time": "2022-02-28T01:46:05.117266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 테스트를 목적으로 하는 문장. [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.decode(hf_tokenizer.encode(\"테스트를 목적으로 하는 문장.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:46:09.039000Z",
     "start_time": "2022-02-28T01:46:08.977106Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/tokenizer_config.json',\n",
       " '../models/special_tokens_map.json',\n",
       " '../models/vocab.txt',\n",
       " '../models/added_tokens.json',\n",
       " '../models/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.save_pretrained(\"../models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:32:00.325233Z",
     "start_time": "2022-03-30T01:32:00.292807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[2, 13964, 7552, 1762, 3, 7451, 5290, 8898, 3]]\n",
      "token_type_ids [[0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[CLS] 정확히 동작하는지 [SEP] 확인하는 테스트 절차 [SEP]\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = ElectraTokenizerFast.from_pretrained(\"../models/\")\n",
    "\n",
    "test = loaded_tokenizer(\n",
    "    [[\"정확히 동작하는지\", \"확인하는 테스트 절차\"]],\n",
    "    max_length=10,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "for k, v in test.items():\n",
    "    print(k, v)\n",
    "\n",
    "for ids in test['input_ids']:\n",
    "    print(loaded_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HoCW3M3_HeeV",
    "M78kc0QyHjW6",
    "ETFAiEUiHgrG"
   ],
   "name": "evolved_transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
