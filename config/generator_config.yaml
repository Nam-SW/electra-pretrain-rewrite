MODEL:
    config:
        embedding_size: 128
        hidden_size: 64
        num_hidden_layers: 12
        num_attention_heads: 4
        intermediate_size: 256
        hidden_act: gelu
        hidden_dropout_prob: 0.1
        attention_probs_dropout_prob: 0.1
        max_position_embeddings: 512
        type_vocab_size: 3
        initializer_range: 0.02
        layer_norm_eps: 1e-12

    tokenizer: path/of/tokenizer
    output_dir: ../generator_model


ARGS:
    training_arguments:
        logging_dir: ../logs
        output_dir: ../ckpt
        
        per_device_train_batch_size: 32
        gradient_accumulation_steps: 4

        num_train_epochs: 1
        # max_steps: 10
        bf16: True

        optim: adamw_torch
        learning_rate: 5e-4
        warmup_ratio: 0.1
        lr_scheduler_type: linear

        # save_strategy: epoch
        save_steps: 100000
        save_total_limit: 10

        logging_steps: 100
        report_to: wandb
        run_name: small++_generator
    
    collator:
        mlm: True
        mlm_probability: 0.15


DATASETS:
    load_args:
        path: private_data_repository
        token: private_hf_token
        path: .
        data_files: 
            train: ../data/*.json

    after_split_train_size: null
    shuffle_seed: 42
    progressbar: True
    max_worker: 64


ETC:
    wandb_project: KorPatElectra


# remove hydra logger
hydra:
    run:
        dir: .
    job:
        chdir: false
    output_subdir: null

defaults:
    - _self_
    - override hydra/hydra_logging: disabled
    - override hydra/job_logging: disabled
